= Testing Guide
:toc:
:toclevels: 3

Comprehensive guide to testing selur.

== Test Categories

selur uses multiple testing strategies:

[cols="1,2,1"]
|===
| Category | Purpose | Location

| **Unit Tests**
| Test individual functions
| `src/lib.rs` (inline)

| **Integration Tests**
| Test WASM bridge end-to-end
| `src/lib.rs` (ignored tests)

| **Doc Tests**
| Test documentation examples
| `src/lib.rs` (doc comments)

| **Benchmarks**
| Measure performance
| `benches/`

| **Examples**
| Test real-world usage
| `examples/`

| **Formal Proofs**
| Mathematical verification
| `idris/`
|===

== Running Tests

=== Quick Test

[source,bash]
----
# Run only unit tests (fast, no WASM needed)
cargo test

# Output:
# running 4 tests
# test tests::test_error_code_conversion ... ok
# test tests::test_error_code_display ... ok
# test tests::test_bridge_load ... ignored
# test tests::test_send_request ... ignored
----

=== Full Test Suite

[source,bash]
----
# Build WASM first
just build

# Run all tests (including ignored)
cargo test -- --include-ignored

# Output shows all tests passing
----

=== Specific Tests

[source,bash]
----
# Run single test
cargo test test_error_code_conversion

# Run tests matching pattern
cargo test error_code

# Run with output
cargo test -- --nocapture

# Run in release mode (faster)
cargo test --release
----

=== Documentation Tests

[source,bash]
----
# Test examples in documentation
cargo test --doc

# Output:
#    Doc-tests selur
# running 1 test
# test src/lib.rs - (line 11) - compile ... ok
----

=== Benchmarks

[source,bash]
----
# Run all benchmarks
cargo bench

# Specific benchmark
cargo bench ipc_benchmark

# Save baseline
cargo bench -- --save-baseline before-optimization

# Compare to baseline
cargo bench -- --baseline before-optimization
----

== Writing Unit Tests

=== Basic Test Structure

[source,rust]
----
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_function_name() {
        // Arrange
        let input = setup_test_data();

        // Act
        let result = function_under_test(input);

        // Assert
        assert_eq!(result, expected_output);
    }
}
----

=== Testing Error Cases

[source,rust]
----
#[test]
#[should_panic(expected = "Invalid input")]
fn test_panics_on_invalid_input() {
    let invalid = create_invalid_data();
    function_that_should_panic(invalid);
}

#[test]
fn test_returns_error() {
    let invalid = create_invalid_data();
    let result = function_that_returns_result(invalid);

    assert!(result.is_err());
    assert_eq!(result.unwrap_err().to_string(), "Expected error message");
}
----

=== Testing ErrorCode

[source,rust]
----
#[test]
fn test_error_code_conversion() {
    assert_eq!(ErrorCode::from_u32(0), Some(ErrorCode::Success));
    assert_eq!(ErrorCode::from_u32(1), Some(ErrorCode::InvalidRequest));
    assert_eq!(ErrorCode::from_u32(2), Some(ErrorCode::ContainerNotFound));
    assert_eq!(ErrorCode::from_u32(3), Some(ErrorCode::PermissionDenied));
    assert_eq!(ErrorCode::from_u32(99), None);
}

#[test]
fn test_error_code_display() {
    assert_eq!(format!("{}", ErrorCode::Success), "Success");
    assert_eq!(format!("{}", ErrorCode::InvalidRequest), "InvalidRequest");
}
----

== Writing Integration Tests

Integration tests require the WASM module.

=== Bridge Loading Test

[source,rust]
----
#[test]
#[ignore]  // Requires WASM file
fn test_bridge_load() {
    let wasm_path = "../zig-out/bin/selur.wasm";
    let bridge = Bridge::new(wasm_path);

    assert!(bridge.is_ok(), "Failed to load WASM: {:?}", bridge.err());

    let mut bridge = bridge.unwrap();
    let memory_size = bridge.memory_size();

    assert!(memory_size.is_ok());
    assert_eq!(memory_size.unwrap(), 1048576); // 1 MB
}
----

=== Request/Response Test

[source,rust]
----
#[test]
#[ignore]
fn test_send_request() {
    let mut bridge = Bridge::new("../zig-out/bin/selur.wasm").unwrap();

    // Create request: CREATE_CONTAINER for "nginx:latest"
    let mut request = Vec::new();
    request.push(0x01);  // Command
    request.extend_from_slice(&12u32.to_le_bytes());  // Length
    request.extend_from_slice(b"nginx:latest");  // Payload

    // Send request
    let response = bridge.send_request(&request);
    assert!(response.is_ok());

    let response = response.unwrap();
    assert!(!response.is_empty());

    // Check status
    let status = ErrorCode::from_u32(response[0] as u32);
    assert_eq!(status, Some(ErrorCode::Success));
}
----

=== Error Handling Test

[source,rust]
----
#[test]
#[ignore]
fn test_oversized_request() {
    let mut bridge = Bridge::new("../zig-out/bin/selur.wasm").unwrap();

    // Create oversized request (>1 MB)
    let huge_request = vec![0u8; 2_000_000];

    let response = bridge.send_request(&huge_request);

    // Should return InvalidRequest error
    assert!(response.is_ok());  // Request succeeds, but returns error code
    let response = response.unwrap();
    assert_eq!(response[0], ErrorCode::InvalidRequest as u8);
}
----

== Property-Based Testing

Use `proptest` for generative testing:

[source,toml]
----
# Add to Cargo.toml
[dev-dependencies]
proptest = "1.0"
----

[source,rust]
----
use proptest::prelude::*;

proptest! {
    #[test]
    fn request_encoding_reversible(data: Vec<u8>) {
        // Property: Encoding and decoding should be inverse operations
        prop_assume!(data.len() <= 1048571);  // Max payload size

        let encoded = encode_request(&data);
        let decoded = decode_request(&encoded)?;

        prop_assert_eq!(data, decoded);
    }

    #[test]
    fn error_codes_are_unique(code1: u8, code2: u8) {
        // Property: Different error codes should not map to same enum value
        let e1 = ErrorCode::from_u32(code1 as u32);
        let e2 = ErrorCode::from_u32(code2 as u32);

        if code1 != code2 && e1.is_some() && e2.is_some() {
            prop_assert_ne!(e1, e2);
        }
    }
}
----

== Writing Benchmarks

Add to `benches/`:

[source,rust]
----
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use selur::Bridge;

fn benchmark_request_sizes(c: &mut Criterion) {
    let mut group = c.benchmark_group("request_processing");

    for size in [100, 1000, 10000].iter() {
        group.bench_with_input(
            BenchmarkId::from_parameter(size),
            size,
            |b, &size| {
                let mut bridge = Bridge::new("../zig-out/bin/selur.wasm").unwrap();
                let request = create_request(size);

                b.iter(|| {
                    bridge.send_request(black_box(&request))
                });
            },
        );
    }

    group.finish();
}

fn create_request(size: usize) -> Vec<u8> {
    let mut req = Vec::new();
    req.push(0x01);
    req.extend_from_slice(&(size as u32).to_le_bytes());
    req.extend_from_slice(&vec![0u8; size]);
    req
}

criterion_group!(benches, benchmark_request_sizes);
criterion_main!(benches);
----

Run:

[source,bash]
----
cargo bench
----

Output includes:
- Time per iteration
- Comparison to previous runs
- Statistical analysis

== Testing Best Practices

=== 1. Test One Thing

[source,rust]
----
// ❌ Bad: Tests multiple things
#[test]
fn test_everything() {
    let bridge = Bridge::new("selur.wasm").unwrap();
    let mem = bridge.memory_size().unwrap();
    assert_eq!(mem, 1048576);

    let req = create_request();
    let resp = bridge.send_request(&req).unwrap();
    assert_eq!(resp[0], 0);
}

// ✅ Good: Separate tests
#[test]
fn test_bridge_creation() {
    let bridge = Bridge::new("selur.wasm");
    assert!(bridge.is_ok());
}

#[test]
fn test_memory_size() {
    let mut bridge = Bridge::new("selur.wasm").unwrap();
    assert_eq!(bridge.memory_size().unwrap(), 1048576);
}
----

=== 2. Use Descriptive Names

[source,rust]
----
// ❌ Bad
#[test]
fn test1() { }

#[test]
fn test_bridge() { }

// ✅ Good
#[test]
fn test_bridge_loads_valid_wasm_file() { }

#[test]
fn test_bridge_returns_error_for_missing_file() { }
----

=== 3. Test Error Paths

[source,rust]
----
#[test]
fn test_invalid_request_format() {
    let mut bridge = Bridge::new("selur.wasm").unwrap();

    // Invalid: Length mismatch
    let mut req = Vec::new();
    req.push(0x01);
    req.extend_from_slice(&100u32.to_le_bytes());  // Says 100 bytes
    req.extend_from_slice(b"short");  // But only 5 bytes

    let resp = bridge.send_request(&req).unwrap();
    assert_eq!(resp[0], ErrorCode::InvalidRequest as u8);
}
----

=== 4. Use Test Fixtures

[source,rust]
----
// Test helpers
fn create_test_bridge() -> Bridge {
    Bridge::new("../zig-out/bin/selur.wasm")
        .expect("WASM file not found. Run: just build")
}

fn create_valid_request(image: &str) -> Vec<u8> {
    let mut req = Vec::new();
    req.push(0x01);
    req.extend_from_slice(&(image.len() as u32).to_le_bytes());
    req.extend_from_slice(image.as_bytes());
    req
}

// Use in tests
#[test]
#[ignore]
fn test_container_creation() {
    let mut bridge = create_test_bridge();
    let request = create_valid_request("nginx:latest");

    let response = bridge.send_request(&request).unwrap();
    assert_eq!(response[0], ErrorCode::Success as u8);
}
----

=== 5. Clean Up Resources

[source,rust]
----
#[test]
fn test_with_cleanup() {
    let temp_file = create_temp_wasm();

    let bridge = Bridge::new(&temp_file);
    assert!(bridge.is_ok());

    // Bridge is dropped here, releasing resources
    drop(bridge);

    // Clean up temp file
    std::fs::remove_file(temp_file).ok();
}
----

== Continuous Integration

Example GitHub Actions workflow:

[source,yaml]
----
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install Zig
        run: |
          wget https://ziglang.org/download/0.16.0-dev/zig-linux-x86_64-0.16.0-dev.tar.xz
          tar xf zig-linux-x86_64-0.16.0-dev.tar.xz
          echo "$PWD/zig-linux-x86_64-0.16.0-dev" >> $GITHUB_PATH

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Install Just
        run: cargo install just

      - name: Build WASM
        run: just build

      - name: Run tests
        run: cargo test --all-features

      - name: Run ignored tests
        run: cargo test -- --ignored

      - name: Run benchmarks (check only)
        run: cargo bench --no-run
----

== Test Coverage

Measure code coverage:

[source,bash]
----
# Install tarpaulin
cargo install cargo-tarpaulin

# Generate coverage report
cargo tarpaulin --out Html

# Open coverage report
open tarpaulin-report.html
----

Aim for:
- **>90% coverage** for critical paths
- **>70% coverage** overall
- **100% coverage** for error handling

== Debugging Failed Tests

=== Enable Logging

[source,bash]
----
RUST_LOG=debug cargo test -- --nocapture
----

=== Run Single Test

[source,bash]
----
cargo test test_name -- --nocapture --test-threads=1
----

=== Use GDB/LLDB

[source,bash]
----
# Build with debug info
cargo test --no-run

# Find test binary
TEST_BIN=$(find target/debug/deps -name 'selur-*' -type f -executable | head -1)

# Debug with GDB
rust-gdb $TEST_BIN
> break test_name
> run test_name
----

== Formal Verification

selur uses Idris2 for formal proofs.

=== Running Proofs

[source,bash]
----
# All proofs
just verify

# Specific proof file
cd idris
idris2 --check Proofs.idr
----

=== Adding New Proofs

[source,idris]
----
-- In idris/Proofs.idr
public export
newProof : (state : SystemState) -> PropertyHolds state
newProof state = ?prove_it

-- Type-check with hole
idris2 --check Proofs.idr
# Shows expected type for ?prove_it

-- Implement proof
newProof state = /* ... proof steps ... */
----

=== Verifying Theorems

[source,idris]
----
-- In idris/Theorems.idr
export
newTheorem : ProofA -> ProofB -> CombinedProperty
newTheorem proofA proofB = /* combine proofs */
----

Verify:

[source,bash]
----
cd idris && idris2 --check Theorems.idr
----

== Test Checklists

Before committing:

- [ ] All unit tests pass: `cargo test`
- [ ] Integration tests pass: `cargo test -- --ignored`
- [ ] Doc tests pass: `cargo test --doc`
- [ ] Examples compile: `cargo build --examples`
- [ ] Benchmarks compile: `cargo bench --no-run`
- [ ] Formal proofs verify: `just verify` (if changed)
- [ ] No compiler warnings: `cargo clippy`
- [ ] Code formatted: `cargo fmt`

Before releasing:

- [ ] All tests pass in release mode: `cargo test --release`
- [ ] Benchmarks show expected performance
- [ ] Coverage >70%: `cargo tarpaulin`
- [ ] No memory leaks: `valgrind` on examples
- [ ] Documentation builds: `cargo doc --no-deps`
- [ ] Examples run successfully
- [ ] Cross-platform tests pass (Linux, macOS, Windows)

== Next Steps

- link:Contributing.adoc[Contributing Guide] - Submit your tested changes
- link:Developer-Guide.adoc[Developer Guide] - Development workflow
- link:Integration-Guide.adoc[Integration Guide] - Test with Svalinn/Vörðr
